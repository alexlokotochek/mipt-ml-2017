{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">First of all -- Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем отличаются современные сверточные сети от сетей 5 летней давности?\n",
    "\n",
    "Они стали очень сильно глубже\n",
    "\n",
    "**Вопрос 2**: Какие неприятности могут возникнуть во время обучения современных нейросетей?\n",
    "\n",
    "Если они слишком глубокие, то может затухать градиент - для этого делают как бы подсети, которые обучаются отдельно (то есть по пути есть какой-то output, который позволяет протолкнуть ошибку чтобы она не затухла к инпуту)\n",
    "\n",
    "**Вопрос 3**: У вас есть очень маленький датасет из 100 картинок, классификация, но вы очень хотите использовать нейросеть, какие неприятности вас ждут и как их решить? что делать если первый вариант  решения не заработает?\n",
    "\n",
    "переобучение (в каждом классе мало элементов, значит будут запоминаться слишком специфические признаки) - можно попробовать добавлять шум на картинки или попытаться как-то их нарезать.\n",
    "- в классе собачек они все на фоне жёлтой стены? вырежем собачек. \n",
    "- в классе котят одни шотландские? пошумим в цветовых каналах или обесцветим\n",
    "- зеркально отразим\n",
    "\n",
    "Если ничего не получится, пойду на толоку и заставлю больше размечать картинки\n",
    "\n",
    "**Вопрос 4**: Как сделать стайл трансфер для музыки? oO\n",
    "\n",
    "Так же, как это делается с картинками. Важно придумать представление для музыки - можно закодировать её в узкую картинку:\n",
    "есть 10^6 частот, возьмём картинку высотой 10^6, в каждом столбике - распределение частот в этот момент времени. Дальше сделаем какой-нибудь антиалиасинг или по-другому пожмём картинку (например, кластеризуем кнн-ом и оставим жирные центры кластеров) - получим опять сильно вытянутую по горизонтали картинку, но уже с адекватной размерностью - можно фигачить перенос стилей:\n",
    "для донора нарежем эту картинку на квадратики (с той же высотой, может быть перекрывающиеся) и сделаем отдельный перенос для каждого квадрата (мб прямоугольника, если жалко ресурсов). будем переносить стили с квадрата-донора на квадрат-получателя (если их не равное колво, можно же прыгать, главное чтобы его номер был первым в том же 0.1-квантиле например)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne import init\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "import lasagne.nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, augment=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        if augment:\n",
    "            # as in paper : \n",
    "            # pad feature arrays with 4 pixels on each side\n",
    "            # and do random cropping of 32x32\n",
    "            padded = np.pad(inputs[excerpt],((0,0),(0,0),(4,4),(4,4)),mode='constant')\n",
    "            random_cropped = np.zeros(inputs[excerpt].shape, dtype=np.float32)\n",
    "            crops = np.random.random_integers(0,high=8,size=(batchsize,2))\n",
    "            for r in range(batchsize):\n",
    "                random_cropped[r,:,:,:] = padded[r,:,crops[r,0]:(crops[r,0]+32),crops[r,1]:(crops[r,1]+32)]\n",
    "            inp_exc = random_cropped\n",
    "        else:\n",
    "            inp_exc = inputs[excerpt]\n",
    "\n",
    "        yield inp_exc, targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=5\n",
    "num_epochs=82\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for j in range(5):\n",
    "    d = unpickle('cifar-10-batches-py/data_batch_'+'j+1')\n",
    "    x = d['data']\n",
    "    y = d['labels']\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "\n",
    "d = unpickle('cifar-10-batches-py/test_batch')\n",
    "xs.append(d['data'])\n",
    "ys.append(d['labels'])\n",
    "\n",
    "x = np.concatenate(xs)/np.float32(255)\n",
    "y = np.concatenate(ys)\n",
    "x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "x -= pixel_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = x[0:50000,:,:,:]\n",
    "Y_train = y[0:50000]\n",
    "X_train_flip = X_train[:,:,:,::-1]\n",
    "Y_train_flip = Y_train\n",
    "X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "\n",
    "X_test = x[50000:,:,:,:]\n",
    "Y_test = y[50000:]\n",
    "\n",
    "data = dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = lasagne.utils.floatX(X_test),\n",
    "        Y_test = Y_test.astype('int32'),)\n",
    "\n",
    "X_train = data['X_train']\n",
    "Y_train = data['Y_train']\n",
    "X_test = data['X_test']\n",
    "Y_test = data['Y_test']\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(l, increase_dim=False, projection=False):\n",
    "    input_num_filters = l.output_shape[1]\n",
    "    if increase_dim:\n",
    "        first_stride = (2,2)\n",
    "        out_num_filters = input_num_filters*2\n",
    "    else:\n",
    "        first_stride = (1,1)\n",
    "        out_num_filters = input_num_filters\n",
    "\n",
    "    stack_1 = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(3,3), stride=first_stride, \n",
    "                                   nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), \n",
    "                                   flip_filters=False))\n",
    "    stack_2 = batch_norm(ConvLayer(stack_1, num_filters=out_num_filters, filter_size=(3,3), stride=(1,1), \n",
    "                                   nonlinearity=None, pad='same', W=lasagne.init.HeNormal(gain='relu'), \n",
    "                                   flip_filters=False))\n",
    "\n",
    "    # add shortcut connections\n",
    "    if increase_dim:\n",
    "        if projection:\n",
    "            # projection shortcut, as option B in paper\n",
    "            projection = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(1,1), \n",
    "                                              stride=(2,2), nonlinearity=None, pad='same', b=None, \n",
    "                                              flip_filters=False))\n",
    "            block = NonlinearityLayer(ElemwiseSumLayer([stack_2, projection]),nonlinearity=rectify)\n",
    "        else:\n",
    "            # identity shortcut, as option A in paper\n",
    "            identity = ExpressionLayer(l, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], s[2]//2, s[3]//2))\n",
    "            padding = PadLayer(identity, [out_num_filters//4,0,0], batch_ndim=1)\n",
    "            block = NonlinearityLayer(ElemwiseSumLayer([stack_2, padding]),nonlinearity=rectify)\n",
    "    else:\n",
    "        block = NonlinearityLayer(ElemwiseSumLayer([stack_2, l]),nonlinearity=rectify)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_in = InputLayer(shape=(None, 3, 32, 32), input_var=input_var)\n",
    "l = batch_norm(ConvLayer(l_in, num_filters=16, \n",
    "                         filter_size=(3,3), stride=(1,1), \n",
    "                         nonlinearity=rectify, pad='same', \n",
    "                         W=lasagne.init.HeNormal(gain='relu'), \n",
    "                         flip_filters=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(n):\n",
    "    l = residual_block(l)\n",
    "l = residual_block(l, increase_dim=True)\n",
    "for _ in range(1,n):\n",
    "    l = residual_block(l)\n",
    "l = residual_block(l, increase_dim=True)\n",
    "for _ in range(1,n):\n",
    "    l = residual_block(l)\n",
    "l = GlobalPoolLayer(l)\n",
    "network = DenseLayer(\n",
    "        l, num_units=10,\n",
    "        W=lasagne.init.HeNormal(),\n",
    "        nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "all_layers = lasagne.layers.get_all_layers(network)\n",
    "l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * 0.0001\n",
    "loss = loss + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "lr = 0.1\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(lr))\n",
    "updates = lasagne.updates.momentum(\n",
    "        loss, params, learning_rate=sh_lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_indices = np.arange(100000)\n",
    "    np.random.shuffle(train_indices)\n",
    "    X_train = X_train[train_indices,:,:,:]\n",
    "    Y_train = Y_train[train_indices]\n",
    "\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, Y_train, 128, shuffle=True, augment=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    if (epoch+1) == 41 or (epoch+1) == 61:\n",
    "        new_lr = sh_lr.get_value() * 0.1\n",
    "        sh_lr.set_value(lasagne.utils.floatX(new_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('cifar10_deep_residual_model.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     with np.load(model) as f:\n",
    "#          param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     lasagne.layers.set_all_param_values(network, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.327143\n",
      "  test accuracy:\t\t92.65 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alaktionov/alaktionov_oneshots/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !mkdir cifar10\n",
    "# !sudo curl -o cifar-10-python.tar.gz https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# !tar -xvzf cifar-10-python.tar.gz -C cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "# from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from cifar import load_CIFAR10\n",
    "# plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "# cifar10_dir = './cifar10/cifar-10-batches-py'\n",
    "# X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def stratified_train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    n_folds = int(1 / test_size)\n",
    "    skf = StratifiedKFold(y, n_folds=n_folds, random_state=random_state)\n",
    "    train_idx, test_idx = iter(skf).next()\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "def load_data(test_size=0.25, random_state=None, \n",
    "              data_dir='/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/data'):\n",
    "    csv_fname = os.path.join(data_dir, 'trainLabels.csv')\n",
    "    df = pd.read_csv(csv_fname)\n",
    "    X = df['id'].apply(lambda i: '%s.png' % i).values\n",
    "    y = LabelEncoder().fit_transform(df['label'].values)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne.handlers import SaveWeights\n",
    "\n",
    "\n",
    "from nolearn_utils.iterators import (\n",
    "    BufferedBatchIteratorMixin,\n",
    "    ReadImageBatchIteratorMixin,\n",
    "    ShuffleBatchIteratorMixin,\n",
    "    RandomFlipBatchIteratorMixin,\n",
    "    AffineTransformBatchIteratorMixin,\n",
    "    make_iterator\n",
    ")\n",
    "from nolearn_utils.hooks import SaveTrainingHistory, PlotTrainingHistory\n",
    "\n",
    "from nolearn_utils.hooks import SaveTrainingHistory, PlotTrainingHistory\n",
    "\n",
    "batch_size = 32\n",
    "n_classes = 10\n",
    "image_size = 32\n",
    "\n",
    "train_iterator_mixins = [\n",
    "    ShuffleBatchIteratorMixin,\n",
    "    ReadImageBatchIteratorMixin,\n",
    "    RandomFlipBatchIteratorMixin,\n",
    "    AffineTransformBatchIteratorMixin,\n",
    "    BufferedBatchIteratorMixin,\n",
    "]\n",
    "TrainIterator = make_iterator('TrainIterator', train_iterator_mixins)\n",
    "\n",
    "test_iterator_mixins = [\n",
    "    ReadImageBatchIteratorMixin,\n",
    "    BufferedBatchIteratorMixin,\n",
    "]\n",
    "TestIterator = make_iterator('TestIterator', test_iterator_mixins)\n",
    "\n",
    "train_iterator_kwargs = {\n",
    "    'buffer_size': 5,\n",
    "    'batch_size': batch_size,\n",
    "    'read_image_size': (image_size, image_size),\n",
    "    'read_image_as_gray': False,\n",
    "    'read_image_prefix_path': '/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/data/train/',\n",
    "    'flip_horizontal_p': 0.5,\n",
    "    'flip_vertical_p': 0,\n",
    "    'affine_p': 0.5,\n",
    "    'affine_scale_choices': np.linspace(0.75, 1.25, 5),\n",
    "    'affine_shear_choices': np.linspace(0.75, 1.25, 5),\n",
    "    'affine_translation_choices': np.arange(-3, 4, 1),\n",
    "    'affine_rotation_choices': np.arange(-45, 50, 5)\n",
    "}\n",
    "train_iterator = TrainIterator(**train_iterator_kwargs)\n",
    "\n",
    "test_iterator_kwargs = {\n",
    "    'buffer_size': 5,\n",
    "    'batch_size': batch_size,\n",
    "    'read_image_size': (image_size, image_size),\n",
    "    'read_image_as_gray': False,\n",
    "    'read_image_prefix_path': '/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/data/train/'\n",
    "}\n",
    "test_iterator = TestIterator(**test_iterator_kwargs)\n",
    "\n",
    "save_weights = SaveWeights('/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/model_weights.pkl', only_best=True, pickle=False)\n",
    "save_training_history = SaveTrainingHistory('/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/model_history.pkl')\n",
    "plot_training_history = PlotTrainingHistory('/home/alaktionov/alaktionov_oneshots/notebooks/examples/cifar10/training_history.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# num_classes = len(classes)\n",
    "# samples_per_class = 7\n",
    "# for y, cls in enumerate(classes):\n",
    "#     idxs = np.flatnonzero(y_train == y)\n",
    "#     idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "#     for i, idx in enumerate(idxs):\n",
    "#         plt_idx = i * num_classes + y + 1\n",
    "#         plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "#         plt.imshow(X_train[idx].astype('uint8').transpose(1, 2, 0))\n",
    "#         plt.axis('off')\n",
    "#         if i == 0:\n",
    "#             plt.title(cls)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from theano import tensor as T\n",
    "from lasagne.nonlinearities import *\n",
    "\n",
    "# input_X = T.tensor4(\"X\")\n",
    "# target_y = T.vector(\"target Y integer\",dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# net = lasagne.layers.InputLayer(shape=(None, 3, 32, 32), input_var=input_X)\n",
    "\n",
    "# # # net = <сверочная нейросеть>\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=32, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=32, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=32, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=48, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=48, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.MaxPool2DLayer(net, \n",
    "#                                     pool_size=(2,2))\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=80, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=80, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=80, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=80, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=80, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.MaxPool2DLayer(net, \n",
    "#                                     pool_size=(2,2))\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=128, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=128, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=128, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=128, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# net = lasagne.layers.Conv2DLayer(net, \n",
    "#                                  num_filters=128, \n",
    "#                                  filter_size=(3,3), \n",
    "#                                  stride=(1,1),\n",
    "#                                  pad=(2,2),\n",
    "#                                  W=init.Orthogonal(2**.5),\n",
    "#                                  nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# # net = lasagne.layers.GlobalPoolLayer(net,\n",
    "# #                                      pool_size=(8,8))\n",
    "\n",
    "# net = lasagne.layers.Pool2DLayer(net, \n",
    "#                                  pool_size=(8,8))\n",
    "\n",
    "\n",
    "# net = lasagne.layers.DenseLayer(net,\n",
    "#                                 num_units=500)\n",
    "\n",
    "# net = lasagne.layers.DenseLayer(net,\n",
    "#                                 num_units=10,\n",
    "#                                 nonlinearity=softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from lasagne.layers.dnn import Conv2DDNNLayer, MaxPool2DDNNLayer\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.layers import Conv2DLayer, MaxPool2DLayer, FeaturePoolLayer, DropoutLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    layers=[\n",
    "        (InputLayer, dict(name='in', shape=(None, 3, image_size, image_size))),\n",
    "\n",
    "        (Conv2DLayer, dict(name='l1c1', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c2', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c3', num_filters=32, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c4', num_filters=48, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l1c5', num_filters=48, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l1p', pool_size=(2,2), stride=(1,1))),\n",
    "\n",
    "        (Conv2DLayer, dict(name='l2c1', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c2', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c3', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c4', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l2c5', num_filters=80, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l2p', pool_size=(2,2), stride=(1,1))),\n",
    "        \n",
    "        (Conv2DLayer, dict(name='l3c1', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c2', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c3', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c4', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (Conv2DLayer, dict(name='l3c5', num_filters=128, filter_size=(3, 3), pad='same', W=init.Orthogonal(2**.5))),\n",
    "        (MaxPool2DLayer, dict(name='l3p', pool_size=(8,8), stride=(1,1))),\n",
    "\n",
    "        (DenseLayer, dict(name='l4', num_units=500)),\n",
    "\n",
    "        (DenseLayer, dict(name='out', num_units=n_classes, nonlinearity=lasagne.nonlinearities.softmax)),\n",
    "    ],\n",
    "    \n",
    "    regression=False,\n",
    "    objective_loss_function=lasagne.objectives.categorical_crossentropy,\n",
    "\n",
    "    update=lasagne.updates.adam,\n",
    "\n",
    "    batch_iterator_train=train_iterator,\n",
    "    batch_iterator_test=test_iterator,\n",
    "\n",
    "    on_epoch_finished=[\n",
    "        save_weights,\n",
    "        save_training_history,\n",
    "        plot_training_history\n",
    "    ],\n",
    "\n",
    "    verbose=10,\n",
    "    max_epochs=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# y_predicted = lasagne.layers.get_output(net)\n",
    "# all_weights = lasagne.layers.get_all_params(net)\n",
    "# print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction = lasagne.layers.get_output(net)\n",
    "# loss = lasagne.objectives.categorical_crossentropy(y_predicted, target_y).mean()\n",
    "\n",
    "# updates = lasagne.updates.adam(loss, all_weights)#, learning_rate=sh_lr)\n",
    "# accuracy = lasagne.objectives.categorical_accuracy(y_predicted, target_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates, allow_input_downcast=True)\n",
    "# accuracy_fun = theano.function([input_X, target_y], accuracy, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот и всё, пошли её учить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "#     assert len(inputs) == len(targets)\n",
    "#     if shuffle:\n",
    "#         indices = np.arange(len(inputs))\n",
    "#         np.random.shuffle(indices)\n",
    "#     for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "#         if shuffle:\n",
    "#             excerpt = indices[start_idx:start_idx + batchsize]\n",
    "#         else:\n",
    "#             excerpt = slice(start_idx, start_idx + batchsize)\n",
    "#         yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процесс обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import sys\n",
    "\n",
    "# num_epochs = 100 #количество проходов по данным\n",
    "\n",
    "# batch_size = 50 #размер мини-батча\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # In each epoch, we do a full pass over the training data:\n",
    "#     train_err = 0\n",
    "#     train_acc = 0\n",
    "#     train_batches = 0\n",
    "#     start_time = time.time()\n",
    "#     for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "#         inputs, targets = batch\n",
    "#         train_err_batch, train_acc_batch = train_fun(inputs, targets)\n",
    "#         train_err += train_err_batch\n",
    "#         train_acc += train_acc_batch\n",
    "#         train_batches += 1\n",
    "\n",
    "#     # And a full pass over the validation data:\n",
    "#     print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "#     print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#     print(\"  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
    "        \n",
    "#     val_acc = 0\n",
    "#     val_batches = 0\n",
    "#     for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "#         inputs, targets = batch\n",
    "#         val_acc += accuracy_fun(inputs, targets)\n",
    "#         val_batches += 1\n",
    "\n",
    "#     # Then we print the results for this epoch:\n",
    "\n",
    "#     print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))\n",
    "    \n",
    "#     sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_acc = 0\n",
    "# test_batches = 0\n",
    "# for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "#     inputs, targets = batch\n",
    "#     acc = accuracy_fun(inputs, targets)\n",
    "#     test_acc += acc\n",
    "#     test_batches += 1\n",
    "# print(\"Final results:\")\n",
    "# print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "#     test_acc / test_batches * 100))\n",
    "\n",
    "# if test_acc / test_batches * 100 > 2.5:\n",
    "#     print \"Achievement unlocked: колдун 80 уровня\"\n",
    "# else:\n",
    "#     print \"Нужно больше магии!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# net.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполните форму\n",
    "\n",
    "https://goo.gl/forms/EeadABISlVmdJqgr2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
